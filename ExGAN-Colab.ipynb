{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ExGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOEeIz6oM2gs"
      },
      "source": [
        "# Needed to plot rainfall maps. Restart runtime after installation (Option in the cell output)\r\n",
        "!apt-get install libgeos-dev\r\n",
        "!pip install https://github.com/matplotlib/basemap/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCFQoEWHN0bG"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from skimage.transform import resize\r\n",
        "import torch.optim as optim\r\n",
        "from torch import LongTensor, FloatTensor\r\n",
        "from scipy.stats import skewnorm, genpareto\r\n",
        "from torchvision.utils import save_image\r\n",
        "import sys\r\n",
        "from datetime import datetime, timedelta\r\n",
        "import torch.utils.data\r\n",
        "import torchvision.utils as vutils\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSJi8wrjlRH"
      },
      "source": [
        "!mkdir data\r\n",
        "!wget https://raw.githubusercontent.com/Stream-AD/ExGAN/master/real.pt\r\n",
        "!cp 'real.pt' data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zJUbnpDua76"
      },
      "source": [
        "from mpl_toolkits.basemap import Basemap, cm\r\n",
        "\r\n",
        "latcorners = np.array([23.476929, 20.741224, 45.43908 , 51.61555 ])\r\n",
        "loncorners = np.array([-118.67131042480469, -82.3469009399414,\r\n",
        "                   -64.52022552490234, -131.4470977783203])\r\n",
        "lon_0 = -105\r\n",
        "lat_0 = 60\r\n",
        "\r\n",
        "def plot_precip(data):\r\n",
        "\t'''\r\n",
        "\tdata is a 813*1051 matrix containing unnormalized precipitation values\r\n",
        "\t'''\r\n",
        "\tif len(data.shape) == 3:\r\n",
        "\t\tdata = data[0]\r\n",
        "\tdata = resize(data, (813, 1051))\r\n",
        "\tdata = (data+1)*50\r\n",
        "\tfig = plt.figure(figsize=(8,8))\r\n",
        "\tax = fig.add_axes([0.1,0.1,0.8,0.8])\r\n",
        "\tm = Basemap(projection='stere',lon_0=lon_0,lat_0=90.,lat_ts=lat_0,\\\r\n",
        "\t            llcrnrlat=latcorners[0],urcrnrlat=latcorners[2],\\\r\n",
        "\t            llcrnrlon=loncorners[0],urcrnrlon=loncorners[2],\\\r\n",
        "\t            rsphere=6371200.,resolution='i', area_thresh=10000)\r\n",
        "\tm.drawcoastlines()\r\n",
        "\tm.drawstates()\r\n",
        "\tm.drawcountries()\r\n",
        "\tm.drawlsmask(land_color=\"#FCF8F3\", ocean_color='#E6FFFF')\r\n",
        "\tparallels = np.arange(0.,90,10.)\r\n",
        "\tm.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\r\n",
        "\tmeridians = np.arange(180.,360.,10.)\r\n",
        "\tm.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\r\n",
        "\tny = data.shape[0]; nx = data.shape[1]\r\n",
        "\tlons, lats = m.makegrid(nx, ny) # get lat/lons of ny by nx evenly space grid.\r\n",
        "\tx, y = m(lons, lats) # compute map proj coordinates.\r\n",
        "\tclevs = np.array([0,1,2.5,5,7.5,10,15,20,30,40,50,70,100,150,200,250,300,400,500,600,750])\r\n",
        "\tcs = m.contourf(x,y,data,clevs,cmap=cm.s3pcpn)\r\n",
        "\tcbar = m.colorbar(cs,location='bottom',pad=\"5%\")\r\n",
        "\tcbar.set_label('mm')\r\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27jZrTV_YGbt"
      },
      "source": [
        "LATENT_DIM = 20\r\n",
        "DATASET_SIZE = 2557\r\n",
        "BETAS = (0.5, 0.999)\r\n",
        "c = 0.75\r\n",
        "k = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvspmnyKd8AT"
      },
      "source": [
        "def extremeness_measure(samples):\r\n",
        "  if len(samples.shape) == 4:\r\n",
        "    return samples.sum(dim=(1, 2, 3)) / 4096\r\n",
        "  else:\r\n",
        "    return samples.sum()/4096"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Gp9V6pOFQR"
      },
      "source": [
        "class NWSDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    NWS Dataset\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self, fake='data/fake.pt', c=0.75, i=0, conditional=False\r\n",
        "    ):\r\n",
        "        self.conditional = conditional\r\n",
        "        self.real = torch.load('data/real.pt')\r\n",
        "        if i > 0:\r\n",
        "          FRAC = int(DATASET_SIZE * (c ** i))\r\n",
        "          self.fake = torch.load(fake)\r\n",
        "          self.data = torch.cat([self.real[:FRAC], self.fake[:DATASET_SIZE-FRAC]], 0)\r\n",
        "        else:\r\n",
        "          self.data = self.real\r\n",
        "        self.data.requires_grad = False\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.data.shape[0]\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        if self.conditional:\r\n",
        "          img = self.data[item]\r\n",
        "          return img, extremeness_measure(img)\r\n",
        "        else:\r\n",
        "          return self.data[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM5jquuKOFzT"
      },
      "source": [
        "def weights_init_normal(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find(\"Conv\") != -1:\r\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\r\n",
        "\r\n",
        "\r\n",
        "def convTINReLU(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.ConvTranspose2d(\r\n",
        "            in_channels,\r\n",
        "            out_channels,\r\n",
        "            kernel_size=kernel_size,\r\n",
        "            stride=stride,\r\n",
        "            padding=padding,\r\n",
        "        ),\r\n",
        "        nn.InstanceNorm2d(out_channels),\r\n",
        "        nn.LeakyReLU(0.2, True),\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "def convINReLU(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(\r\n",
        "            in_channels,\r\n",
        "            out_channels,\r\n",
        "            kernel_size=kernel_size,\r\n",
        "            stride=stride,\r\n",
        "            padding=padding,\r\n",
        "        ),\r\n",
        "        nn.InstanceNorm2d(out_channels),\r\n",
        "        nn.LeakyReLU(0.2, True),\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class GeneratorUnconditional(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super(GeneratorUnconditional, self).__init__()\r\n",
        "        self.block1 = convTINReLU(in_channels, 512, 4, 1, 0)\r\n",
        "        self.block2 = convTINReLU(512, 256)\r\n",
        "        self.block3 = convTINReLU(256, 128)\r\n",
        "        self.block4 = convTINReLU(128, 64)\r\n",
        "        self.block5 = nn.ConvTranspose2d(64, out_channels, 4, 2, 1)\r\n",
        "\r\n",
        "    def forward(self, inp):\r\n",
        "        out = self.block1(inp)\r\n",
        "        out = self.block2(out)\r\n",
        "        out = self.block3(out)\r\n",
        "        out = self.block4(out)\r\n",
        "        return torch.tanh(self.block5(out))\r\n",
        "\r\n",
        "\r\n",
        "class DiscriminatorUnconditional(nn.Module):\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(DiscriminatorUnconditional, self).__init__()\r\n",
        "        self.block1 = convINReLU(in_channels, 64)\r\n",
        "        self.block2 = convINReLU(64, 128)\r\n",
        "        self.block3 = convINReLU(128, 256)\r\n",
        "        self.block4 = convINReLU(256, 512)\r\n",
        "        self.block5 = nn.Conv2d(512, 64, 4, 1, 0)\r\n",
        "        self.source = nn.Linear(64, 1)\r\n",
        "\r\n",
        "    def forward(self, inp):\r\n",
        "        out = self.block1(inp) \r\n",
        "        out = self.block2(out)\r\n",
        "        out = self.block3(out)\r\n",
        "        out = self.block4(out)\r\n",
        "        out = self.block5(out)\r\n",
        "        size = out.shape[0]\r\n",
        "        out = out.view(size, -1)\r\n",
        "        source = torch.sigmoid(self.source(out))\r\n",
        "        return source"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqVolE84Tiv_"
      },
      "source": [
        "def getTrueFalseTensors(batch_size):\r\n",
        "  trueTensor = 0.7+0.5*torch.rand((batch_size, 1))\r\n",
        "  falseTensor = 0.3*torch.rand((batch_size, 1))\r\n",
        "  probFlip = torch.rand((batch_size, 1)) < 0.05\r\n",
        "  probFlip = probFlip.float()\r\n",
        "  trueTensor, falseTensor = (\r\n",
        "      probFlip * falseTensor + (1 - probFlip) * trueTensor,\r\n",
        "      probFlip * trueTensor + (1 - probFlip) * falseTensor,\r\n",
        "  )\r\n",
        "  return trueTensor.cuda(), falseTensor.cuda()\r\n",
        "  \r\n",
        "def trainGAN(dataloader, Generator, Discriminator, optimizerGenerator, optimizerDiscrimintor, noise=0):\r\n",
        "  for images in dataloader:\r\n",
        "      batch_size = images[0].shape[0]\r\n",
        "      trueTensor, falseTensor = getTrueFalseTensors(batch_size)\r\n",
        "      images = images.cuda()\r\n",
        "      realSource = Discriminator(images + noise*torch.randn_like(images).cuda())\r\n",
        "      realLoss = criterionSource(realSource, trueTensor.expand_as(realSource))\r\n",
        "      latent = torch.randn(batch_size, LATENT_DIM, 1, 1).cuda()\r\n",
        "      fakeData = Generator(latent)\r\n",
        "      fakeSource = Discriminator(fakeData.detach())\r\n",
        "      fakeLoss = criterionSource(fakeSource, falseTensor.expand_as(fakeSource))\r\n",
        "      lossD = realLoss + fakeLoss\r\n",
        "      optimizerDiscrimintor.zero_grad()\r\n",
        "      lossD.backward()\r\n",
        "      torch.nn.utils.clip_grad_norm_(Discriminator.parameters(),20)\r\n",
        "      optimizerDiscrimintor.step()\r\n",
        "      fakeSource = Discriminator(fakeData)\r\n",
        "      trueTensor = 0.9*torch.ones((batch_size, 1)).cuda()\r\n",
        "      lossG = criterionSource(fakeSource, trueTensor.expand_as(fakeSource))\r\n",
        "      optimizerGenerator.zero_grad()\r\n",
        "      lossG.backward()\r\n",
        "      torch.nn.utils.clip_grad_norm_(Generator.parameters(),20)\r\n",
        "      optimizerGenerator.step()\r\n",
        "      return lossG.item(), lossD.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULjDSDxHNc8U"
      },
      "source": [
        "dataloader = DataLoader(NWSDataset(), batch_size=256, shuffle=True)\r\n",
        "\r\n",
        "criterionSource = nn.BCELoss()\r\n",
        "criterionContinuous = nn.L1Loss()\r\n",
        "criterionValG = nn.L1Loss()\r\n",
        "criterionValD = nn.L1Loss()\r\n",
        "UnconditionalG = GeneratorUnconditional(in_channels=LATENT_DIM, out_channels=1).cuda()\r\n",
        "UnconditionalD = DiscriminatorUnconditional(in_channels=1).cuda()\r\n",
        "UnconditionalG.apply(weights_init_normal)\r\n",
        "UnconditionalD.apply(weights_init_normal)\r\n",
        "\r\n",
        "optimizerG = optim.Adam(UnconditionalG.parameters(), lr=2e-4, betas=BETAS)\r\n",
        "optimizerD = optim.Adam(UnconditionalD.parameters(), lr=1e-4, betas=BETAS)\r\n",
        "static_z = FloatTensor(torch.randn((81, LATENT_DIM, 1, 1))).cuda()\r\n",
        "\r\n",
        "DIRNAME = 'DCGAN/'\r\n",
        "os.makedirs(DIRNAME, exist_ok=True)\r\n",
        "tk = tqdm(range(1000))\r\n",
        "for epoch in tk:\r\n",
        "    noise = 1e-5*max(1 - (epoch/500.0), 0)\r\n",
        "    lossG, lossD = trainGAN(dataloader, UnconditionalG, UnconditionalD, optimizerG, optimizerD, noise=0)\r\n",
        "    tk.set_postfix(lossG=lossG, lossD=lossD)\r\n",
        "UnconditionalG.eval()\r\n",
        "with torch.no_grad():\r\n",
        "    fakeSamples = UnconditionalG(Variable(torch.randn(int(DATASET_SIZE/c), LATENT_DIM, 1, 1)).cuda()).cpu()\r\n",
        "sorted_indices = extremeness_measure(fakeSamples).numpy().argsort()[::-1].copy()\r\n",
        "UnconditionalG.train()\r\n",
        "torch.save(fakeSamples[sorted_indices], 'data/fake.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LSBy30sP0Wq"
      },
      "source": [
        "plot_precip(fakeSamples[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYE-egU4PXnS"
      },
      "source": [
        "optimizerG = optim.Adam(UnconditionalG.parameters(), lr=2e-5, betas=BETAS)\r\n",
        "optimizerD = optim.Adam(UnconditionalD.parameters(), lr=1e-5, betas=BETAS)\r\n",
        "\r\n",
        "c = 0.75\r\n",
        "k = 10\r\n",
        "DIRNAME = 'DistShift/'\r\n",
        "os.makedirs(DIRNAME, exist_ok=True)\r\n",
        "\r\n",
        "fake_name = 'data/fake.pt'\r\n",
        "for i in range(1, k):\r\n",
        "    print(\"Distribution Shift: Iteration \", i)\r\n",
        "    dataloader = DataLoader(NWSDataset(fake=fake_name, c=c, i=i), batch_size=256, shuffle=True)\r\n",
        "    tk = tqdm(range(0, 100))\r\n",
        "    for epoch in tk:\r\n",
        "        lossG, lossD = trainGAN(dataloader, UnconditionalG, UnconditionalD, optimizerG, optimizerD)\r\n",
        "        tk.set_postfix(lossG=lossG, lossD=lossD)\r\n",
        "    with torch.no_grad():\r\n",
        "        UnconditionalG.eval()\r\n",
        "        fsize = int((1 - (c ** (i + 1))) * DATASET_SIZE / c)\r\n",
        "        fakeSamples = UnconditionalG(torch.randn(fsize, LATENT_DIM, 1, 1).cuda()).cpu()\r\n",
        "        sorted_indices = extremeness_measure(fakeSamples).numpy().argsort()[::-1].copy()\r\n",
        "        fake_name = DIRNAME + 'fake' + str(i + 1) + '.pt'\r\n",
        "        torch.save(fakeSamples.data[sorted_indices], fake_name)\r\n",
        "        UnconditionalG.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rkrFn_1QynO"
      },
      "source": [
        "plot_precip(torch.load(DIRNAME+'fake2.pt')[1000]) #Sorted by extremeness. Hence, looking at the middle elements. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXN9Q3Q0Q8tB"
      },
      "source": [
        "plot_precip(torch.load(DIRNAME+'fake'+str(k)+'.pt')[1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mYlvNqAW9ER"
      },
      "source": [
        "dataset = NWSDataset(fake='DistShift/fake'+str(k)+'.pt', c=c, i=k, conditional=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59uuZBRCoU23"
      },
      "source": [
        "measures = extremeness_measure(dataset.data)\r\n",
        "threshold = measures.min() # Already tail of the data\r\n",
        "tail = measures[np.where(measures > threshold)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuAWGTzEbOKD"
      },
      "source": [
        "genpareto_params = genpareto.fit(tail-threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCaGGW7ZUf9s"
      },
      "source": [
        "class GeneratorConditional(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super(GeneratorConditional, self).__init__()\r\n",
        "        self.block1 = convTINReLU(in_channels + 1, 512, 4, 1, 0)\r\n",
        "        self.block2 = convTINReLU(512, 256)\r\n",
        "        self.block3 = convTINReLU(256, 128)\r\n",
        "        self.block4 = convTINReLU(128, 64)\r\n",
        "        self.block5 = nn.ConvTranspose2d(64, out_channels, 4, 2, 1)\r\n",
        "\r\n",
        "    def forward(self, latent, continuous_code):\r\n",
        "        inp = torch.cat((latent, continuous_code), 1)\r\n",
        "        out = self.block1(inp)\r\n",
        "        out = self.block2(out)\r\n",
        "        out = self.block3(out)\r\n",
        "        out = self.block4(out)\r\n",
        "        return torch.tanh(self.block5(out))\r\n",
        "\r\n",
        "class DiscriminatorConditional(nn.Module):\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(DiscriminatorConditional, self).__init__()\r\n",
        "        self.block1 = convINReLU(in_channels, 64)\r\n",
        "        self.block2 = convINReLU(64, 128)\r\n",
        "        self.block3 = convINReLU(128, 256)\r\n",
        "        self.block4 = convINReLU(256, 512)\r\n",
        "        self.block5 = nn.Conv2d(512, 64, 4, 1, 0)\r\n",
        "        self.source = nn.Linear(64 + 1, 1)\r\n",
        "\r\n",
        "    def forward(self, inp, extreme):\r\n",
        "        sums = extremeness_measure(inp)\r\n",
        "        diff = torch.abs(extreme.view(-1, 1) - sums.view(-1, 1)) / torch.abs(extreme.view(-1, 1))\r\n",
        "        out = self.block1(inp)\r\n",
        "        out = self.block2(out)\r\n",
        "        out = self.block3(out)\r\n",
        "        out = self.block4(out)\r\n",
        "        out = self.block5(out)\r\n",
        "        size = out.shape[0]\r\n",
        "        out = out.view(size, -1)\r\n",
        "        source = torch.sigmoid(self.source(torch.cat([out, diff], 1)))\r\n",
        "        return source\r\n",
        "\r\n",
        "\r\n",
        "criterionSource = nn.BCELoss()\r\n",
        "G = GeneratorConditional(in_channels=LATENT_DIM, out_channels=1).cuda()\r\n",
        "D = DiscriminatorConditional(in_channels=1).cuda()\r\n",
        "G.apply(weights_init_normal)\r\n",
        "D.apply(weights_init_normal)\r\n",
        "\r\n",
        "rv = genpareto(*genpareto_params)\r\n",
        "\r\n",
        "c = 0.75\r\n",
        "k = 10\r\n",
        "\r\n",
        "def sample_genpareto(size):\r\n",
        "    probs = torch.rand(size)\r\n",
        "    return FloatTensor(rv.ppf(probs)) + threshold\r\n",
        "\r\n",
        "\r\n",
        "optimizerG = optim.Adam(G.parameters(), lr=2e-4, betas=BETAS)\r\n",
        "optimizerD = optim.Adam(D.parameters(), lr=1e-4, betas=BETAS)\r\n",
        "static_code = sample_genpareto((81, 1, 1, 1)).cuda()\r\n",
        "static_z = FloatTensor(torch.randn((81, LATENT_DIM, 1, 1))).cuda()\r\n",
        "    \r\n",
        "def sample_image(batches_done):\r\n",
        "    static_sample = G(static_z, static_code).cpu()\r\n",
        "    static_sample = (static_sample + 1) / 2.0\r\n",
        "    save_image(static_sample, DIRNAME + \"%d.png\" % batches_done, nrow=9)\r\n",
        "\r\n",
        "DIRNAME = 'ExGAN/'\r\n",
        "os.makedirs(DIRNAME, exist_ok=True)\r\n",
        "fakename = 'DistShift/fake'+str(k)+'.pt'\r\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\r\n",
        "tk = tqdm(range(0, 1)) # Actual Number of Epochs is 1000\r\n",
        "for epoch in tk:\r\n",
        "    noise = 1e-5 * max(1 - (epoch / 1000.0), 0)\r\n",
        "    for images, labels in dataloader:\r\n",
        "        batch_size = images.shape[0]\r\n",
        "        trueTensor, falseTensor = getTrueFalseTensors(batch_size)\r\n",
        "        images, labels = images.cuda(), labels.view(-1, 1).cuda()\r\n",
        "        realSource = D(images, labels)\r\n",
        "        realLoss = criterionSource(realSource, trueTensor.expand_as(realSource))\r\n",
        "        latent = torch.randn(batch_size, LATENT_DIM, 1, 1).cuda()\r\n",
        "        code = sample_genpareto((batch_size, 1, 1, 1)).cuda()\r\n",
        "        fakeGen = G(latent, code)\r\n",
        "        fakeGenSource = D(fakeGen.detach(), code)\r\n",
        "        fakeGenLoss = criterionSource(fakeGenSource, falseTensor.expand_as(fakeGenSource))\r\n",
        "        lossD = realLoss + fakeGenLoss\r\n",
        "        optimizerD.zero_grad()\r\n",
        "        lossD.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(D.parameters(), 20)\r\n",
        "        optimizerD.step()\r\n",
        "        fakeGenSource = D(fakeGen, code)\r\n",
        "        fakeLabels = extremeness_measure(fakeGen)\r\n",
        "        L_ext = torch.mean(torch.abs((fakeLabels - code.view(batch_size)) / code.view(batch_size)))\r\n",
        "        lossG = criterionSource(fakeGenSource, trueTensor.expand_as(fakeGenSource)) + L_ext\r\n",
        "        optimizerG.zero_grad()\r\n",
        "        lossG.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(G.parameters(), 20)\r\n",
        "        optimizerG.step()\r\n",
        "    tk.set_postfix(lossG=lossG.item(), lossD=lossD.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1RSvrm-nKcJ"
      },
      "source": [
        "# Takes around 1.5 hrs for training 1000 epochs. Use the pretrained weights instead.\r\n",
        "!wget https://raw.githubusercontent.com/Stream-AD/ExGAN/master/ExGANweights.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOQgI2hYl0rO"
      },
      "source": [
        "G.load_state_dict(torch.load('ExGANweights.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw1ZEuTDadmS"
      },
      "source": [
        "G.eval()\r\n",
        "tau = 1e-4\r\n",
        "tau_prime = tau/c**k\r\n",
        "val = rv.ppf((1-tau_prime)) + threshold\r\n",
        "code = torch.ones(100, 1, 1, 1).cuda()*val\r\n",
        "latent = torch.randn((100, LATENT_DIM, 1, 1)).cuda()\r\n",
        "with torch.no_grad():\r\n",
        "  images = G(latent, code).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhfsXOrNa9dJ"
      },
      "source": [
        "plot_precip(images[0]) # Feel free to change tau, and look at more samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIEKFX0ztp1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
